# FAQ for `mfdn-transitions` #

Patrick J. Fasano, as told to Mark A. Caprio
University of Notre Dame

+ 07/16/21 (mac): Created.

----------------------------------------------------------------

## HPC run parameters ##

Q: How many ranks (and threads per rank) should I use?

A: First it is useful to understand how work is split up among ranks.  The bra
and ket are broken up into subvectors, based on the number of "diagonal"
processes in use when they were generated by `mfdn`.  Thus, if there were n
diagonal processes in the `mfdn` run for the ket, and m in the run for the bra,
then the matrix element calculation in `mfdn-transitions` then the matrix
representing the operator in `mfdn-transitions` is effectively broken into mn
tiles, between these pairs of subvectors.  The contributions of these tiles to
the final total matrix element are calculated independently, and are distributed
round-robin over the MPI ranks in `mfdn-transitions`.  While there are no rigid
constraints on choosing the number N of ranks, there are a few considerations:

- Strong scaling is generally good (except as noted below).

- You want the round-robin distribution to be efficient.  Assuming the
  calculation time to be the same for all tiles, this suggests choosing N as
  a divisor of mn, so that no nodes will be left idle in the final round of
  distribution.  (Corollary: N=mn is the maximum useful number of ranks, as
  N>mn will leave some ranks with no work whatsoever.)

- Calculation time for each tile is typically short (less than a minute)
  compared to the overhead of launching an MPI process (as well as the time to
  run the serial setup codes to generate the OBMEs/TBMEs, if you are doing that
  as part of the same batch job).  So, while choosing N=mn ranks may minimize
  wall time for the actual computation, it will be extremely wasteful of CPU
  time, as most of the billable time will be in MPI setup (and serial setup
  tasks).  For large runs, it is therefore generally better to have N<<mn, to
  minimize MPI overhead relative to actual computation.

- The memory requirements per rank are modest: storage for one ket subvector,
  multiple bra subvectors, and the TBMEs for any two-body operators.  This is
  usually not a limiting factor.

- As usual on HPC systems with a multilayer memory hierarchy, it is probably
  prudent to respect memory boundaries (e.g., sockets or NUMA domains).  Thus,
  as with `mfdn`, you may wish to allocate multiple ranks per node, each
  receiving their proportionate "share" of the total available threads
  efficiently supported by the node (logical cores).

As a rule of thumb, based on the above considerations, N=m or n ranks is a
reasonable choice.

Example: Alice generated Nmax06 wave functions for 12C, and Bob wants to
calculate transisitions.

Alice generated the wave functions using `mfdn` on Cori KNL.  Based on memory
needs, she chose 11 "diagonal" ranks.  There were thus 11*(11+1)/2=66 total
ranks, that is, one for each block of the upper triangle of the Hamiltonian
matrix.

That's all Bob needs to know about the wave functions to set up the
`mfdn-transitions` run.  But Bob will also be running on Cori KNL, so let us
digress for a moment to review how Alice took the KNL node configuration into
account.  Since the KNL nodes are broken into 4 NUMA domains, she split these 66
ranks over 17 nodes.  With 4x hyperthreading, and using 64 of the KNL
processor's physical cores (to leave a couple in reserve for system tasks),
there are 256 logical cores available per KNL node.  So, splitting those over 4
ranks, she ran with 64 threads per rank.  Alice was using the `mcscript` job
submission tool `qsubm`, so her submission parameters were `--ranks=66 --nods=17
--threads=64` (and `--serialthreads=256` for the "serial", i.e.,
OpenMP-parallelized, setup codes from `shell`, which are run before either
`mfdn` or `mfdn-transitions` to generate the OBME/TBME files).

Bob then could therfore reasonably choose anywhere from 1 to 17^2=289 ranks.  If
he were to chose 1 rank, he would let it use all available hyperthreads, with
`--threads=256` (and, again, in this and the following examples, always with
`--serialthreads=256` for the OBME/TBME setup codes).  But instead he follows
the rule of thumb above and chooses to run on 17 ranks.  If he respects the NUMA
domains, he will put 4 ranks per node, so his `qsubm` submission parameters are
`--ranks=17 --nodes=5 --threads=64`.
